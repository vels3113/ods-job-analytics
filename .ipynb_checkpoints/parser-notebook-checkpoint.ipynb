{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating raw jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading raw json\n",
    "raw_json_file_names = []\n",
    "raw_jsons = []\n",
    "\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.startswith(\"run_results_\"):\n",
    "        raw_json_file_names.append(file)\n",
    "        \n",
    "for file in raw_json_file_names:\n",
    "    with open(file,'r') as filestream:\n",
    "        raw_jsons.append(pd.read_json(filestream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnessasary stuff\n",
    "for json_table in raw_jsons:\n",
    "    del json_table['remove']\n",
    "    del json_table['listingValue']\n",
    "    json_table.columns = ['html']\n",
    "    json_table['html_str'] = json_table['html'].astype(str)\n",
    "    json_table = json_table.drop_duplicates(subset='html_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating jsons and removing duplicates\n",
    "json_all = raw_jsons[0]\n",
    "for json_table in raw_jsons[1:]:\n",
    "    json_all = pd.concat([json_all,json_table], axis=0, ignore_index=True)\n",
    "json_all = json_all.drop_duplicates(subset=\"html_str\")\n",
    "del json_all[\"html_str\"]\n",
    "json_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output raw json without duplicate as dataframe in pickle\n",
    "with open('raw_json_no_dups.json','w') as fout:\n",
    "    json_all.to_json(fout,orient='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting only user messages\n",
    "Also parsing links to message and authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to add empty string in case of empty search\n",
    "link_search = lambda s: '' if s is None else s['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap user messages \n",
    "def scrapFromHTML(soup, row):\n",
    "    row['author'] = \"https://opendatascience.slack.com\" + link_search(soup.find(\"a\", class_=\"c-link c-message__sender_link\"))\n",
    "    row['link'] = link_search(soup.find(\"a\", class_=\"c-link c-timestamp c-timestamp--static\"))\n",
    "    tag = soup.find(\"span\", class_=\"c-message__body\")\n",
    "    if not tag is None:\n",
    "        row['message'] = tag.get_text(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load raw table\n",
    "with open('raw_json_no_dups.json','r') as fin:\n",
    "    html_table = pd.read_json(fin,compression=None,orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add tables for future scrapped data\n",
    "html_table['author'] = \"\"\n",
    "html_table['link'] = \"\"\n",
    "html_table['message'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping of new information\n",
    "for i in html_table.index:\n",
    "    html_doc = html_table['html'][i]['name']\n",
    "    soup = BeautifulSoup(html_doc,'html5lib')\n",
    "    scrapFromHTML(soup, html_table.loc[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy table without non-user messages\n",
    "html_table_usr_msg = html_table[html_table.author!=\"https://opendatascience.slack.com\"]\n",
    "html_table_usr_msg.reset_index(drop=True, inplace=True)\n",
    "del html_table_usr_msg['html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output user messages table\n",
    "with open(\"user_messages.json\",'w') as fout:\n",
    "    html_table_usr_msg.to_json(fout, compression=None, orient='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting numbers out of text\n",
    "And removing messages without numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input user messages table\n",
    "with open(\"user_messages.json\",'r') as fin:\n",
    "    usr_msg = pd.read_json(fin, compression=None, orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_msg['numbers'] = usr_msg.apply(lambda x: [], axis=1)\n",
    "usr_msg['hasNumbers'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#scrapping numbers in messages\n",
    "for i in usr_msg.index:\n",
    "    msg = usr_msg['message'][i]\n",
    "    usr_msg['numbers'][i] = re.findall(r'[0-9]+', msg)\n",
    "    if usr_msg['numbers'][i]:\n",
    "        usr_msg['hasNumbers'][i] = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy table without only-word messages (other are candidates for having vacancy information)\n",
    "usr_msg_clear = usr_msg[usr_msg.hasNumbers]\n",
    "usr_msg_clear.reset_index(drop=True, inplace=True)\n",
    "del usr_msg_clear['hasNumbers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output clear user messages table\n",
    "with open(\"user_messages_clear.json\",'w') as fout:\n",
    "    usr_msg_clear.to_json(fout, compression=None, orient='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing data and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapDateTime(row, link):\n",
    "\n",
    "    usr = \"ksetherg@gmail.com\"\n",
    "    pwd = \"l'dbl1996\"\n",
    "\n",
    "    options = webdriver.chrome.options.Options()\n",
    "    options.headless = True\n",
    "    \n",
    "    driver = webdriver.Chrome('/opt/chromedriver', options=options)\n",
    "    driver.get(link.replace('archives','messages'))\n",
    "\n",
    "    elem = driver.find_element_by_id(\"email\")\n",
    "    elem.send_keys(usr)\n",
    "    elem = driver.find_element_by_id(\"password\")\n",
    "    elem.send_keys(pwd)\n",
    "    elem.send_keys(Keys.RETURN)\n",
    "#    elem.send_keys(\"Posted using Python's Selenium WebDriver bindings!\")\n",
    "#    elem = driver.find_element_by_id(\"signin_btn\")\n",
    "#    elem.click()\n",
    "    try:\n",
    "        WebDriverWait(driver,15).until(\n",
    "             EC.presence_of_element_located((By.CLASS_NAME,'p-archives_banner')))\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        WebDriverWait(driver,5).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME,'c-message_list__day_divider__label__pill')))\n",
    "        elem1 = driver.find_element_by_class_name('c-message_list__day_divider__label__pill')\n",
    "        row['date'] = elem1.text\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    except StaleElementReferenceException:\n",
    "        elem1 = driver.find_element_by_class_name('c-message_list__day_divider__label__pill')\n",
    "        row['date'] = elem1.text\n",
    "        \n",
    "    try:\n",
    "        WebDriverWait(driver,5).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME,'c-message__content_header')))\n",
    "        elems = driver.find_elements_by_class_name('c-message__content_header')\n",
    "        for elem in elems:\n",
    "            if link in elem.get_attribute('innerHTML'):\n",
    "                row['time'] = re.search('data-stringify-suffix=\"]\"(.+?)</span>', elem.get_attribute('innerHTML')).group(1)\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    except StaleElementReferenceException:\n",
    "        elems = driver.find_elements_by_class_name('c-message__content_header')\n",
    "        for elem in elems:\n",
    "            if link in elem.get_attribute('innerHTML'):\n",
    "                row['time'] = re.search('data-stringify-suffix=\"]\"(.+?)</span>', elem.get_attribute('innerHTML')).group(1)\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input user messages table\n",
    "with open(\"user_messages_clear.json\",'r') as fin:\n",
    "    usr_msg_clear = pd.read_json(fin, compression=None, orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_msg_clear['date'] = ''\n",
    "usr_msg_clear['time'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/244 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(usr_msg_clear.index[::10]):\n",
    "    threads = []\n",
    "    for j in range(i,min(i + 10,usr_msg_clear.index[-1])):\n",
    "        thread = threading.Thread(target=scrapDateTime, args=[usr_msg_clear.loc[j,:], usr_msg_clear['link'][j]])\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        \n",
      "1        \n",
      "2        \n",
      "3        \n",
      "4        \n",
      "5        \n",
      "6        \n",
      "7        \n",
      "8        \n",
      "9        \n",
      "10       \n",
      "11       \n",
      "12       \n",
      "13       \n",
      "14       \n",
      "15       \n",
      "16       \n",
      "17       \n",
      "18       \n",
      "19       \n",
      "20       \n",
      "21       \n",
      "22       \n",
      "23       \n",
      "24       \n",
      "25       \n",
      "26       \n",
      "27       \n",
      "28       \n",
      "29       \n",
      "       ..\n",
      "2402     \n",
      "2403     \n",
      "2404     \n",
      "2405     \n",
      "2406     \n",
      "2407     \n",
      "2408     \n",
      "2409     \n",
      "2410     \n",
      "2411     \n",
      "2412     \n",
      "2413     \n",
      "2414     \n",
      "2415     \n",
      "2416     \n",
      "2417     \n",
      "2418     \n",
      "2419     \n",
      "2420     \n",
      "2421     \n",
      "2422     \n",
      "2423     \n",
      "2424     \n",
      "2425     \n",
      "2426     \n",
      "2427     \n",
      "2428     \n",
      "2429     \n",
      "2430     \n",
      "2431     \n",
      "Name: date, Length: 2432, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(usr_msg_clear['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlackUser:\n",
    "\n",
    "    def __init__(self, link='https://opendatascience.slack.com/archives/C04DA5FUF', filename='slack_dump.json'):\n",
    "        self.data = {}\n",
    "        self.channel_url = link\n",
    "        self.filename = filename\n",
    "\n",
    "        self._session: Optional[aiohttp.ClientSession] = None\n",
    "\n",
    "    @property\n",
    "    def session(self) -> aiohttp.ClientSession:\n",
    "        if self._session is None:\n",
    "            self._session = aiohttp.ClientSession()\n",
    "        return self._session\n",
    "\n",
    "    async def close(self):\n",
    "        if self._session is not None:\n",
    "            await self.session.close()\n",
    "\n",
    "    def dump_exist(self, filename: str = None):\n",
    "        return is_non_zero_file(filename or self.filename)\n",
    "\n",
    "    def load(self, filename: str = None):\n",
    "        with open(filename or self.filename, 'r', encoding='utf-8') as file:\n",
    "            self.data = json.load(file)\n",
    "\n",
    "    def dump(self, filename: str = None):\n",
    "        with open(filename or self.filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(self.data, file, indent=True, ensure_ascii=False)\n",
    "\n",
    "    async def request(self, url, method='GET', data=None, headers=None):\n",
    "        async with self.session.request(method, url, data=data, headers=headers) as response:\n",
    "            return await response.read()\n",
    "\n",
    "    async def authorization_on_slack(self, username, password):\n",
    "        login_url = self.channel_url\n",
    "        login_page = await self.request(login_url)\n",
    "        auth_token = BeautifulSoup(login_page, features='lxml').find('input', {'name': 'crumb'})['value']\n",
    "\n",
    "        data = {'_username': username, '_password': password, '_remember_me': 'on', 'crumb': auth_token}\n",
    "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "        await self.request(f'{self.lmsu_url}/login_check', method='POST', data=data, headers=headers)\n",
    "\n",
    "    async def scrap_user(self, user_id):\n",
    "        response = await self.request(f'{self.lmsu_url}/rus/user/achievement/user/{user_id}/list')\n",
    "        soup = BeautifulSoup(response, features=\"lxml\")\n",
    "        name_field = soup.find('h3', {'class': 'achievements-user__name'}) or \\\n",
    "        soup.find('a', {'class': 'user-block__link user-block__link--logged'})\n",
    "        user_data = {'name': name_field.text.strip(), 'achievements': []}\n",
    "        subsection(f'Processing user \\\"{user_data[\"name\"]}\\\"')\n",
    "        for achievement in soup.find_all(\"article\", {\"class\": \"achievement\"}):\n",
    "            curr_data = {\n",
    "                          'title': achievement.find(\"a\", {\"class\": \"achievement__link\"}).text.strip(),\n",
    "                          'category': achievement.find(\"p\", {\"class\": \"achievement__more\"}).text.strip(),\n",
    "                          'score': int(achievement.find(\"span\", {\"class\": \"ach-pill\"}).text),\n",
    "                          'checked': bool(achievement.find(\"input\", {\"checked\": \"checked\"})),\n",
    "                          'url': self.lmsu_url + achievement.find(\"a\", {\"class\": \"achievement__link\"})['href'],\n",
    "                          'date': '',\n",
    "                          'file': '',\n",
    "                          'comment': '',\n",
    "                          'comment_our': '',\n",
    "                        }\n",
    "            user_data['achievements'].append(curr_data)\n",
    "        return user_id, user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
