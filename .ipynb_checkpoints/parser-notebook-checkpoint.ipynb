{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating raw jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading raw json\n",
    "raw_json_file_names = []\n",
    "raw_jsons = []\n",
    "\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.startswith(\"run_results_\"):\n",
    "        raw_json_file_names.append(file)\n",
    "        \n",
    "for file in raw_json_file_names:\n",
    "    with open(file,'r') as filestream:\n",
    "        raw_jsons.append(pd.read_json(filestream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnessasary stuff\n",
    "for json_table in raw_jsons:\n",
    "    del json_table['remove']\n",
    "    del json_table['listingValue']\n",
    "    json_table.columns = ['html']\n",
    "    json_table['html_str'] = json_table['html'].astype(str)\n",
    "    json_table = json_table.drop_duplicates(subset='html_str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating jsons and removing duplicates\n",
    "json_all = raw_jsons[0]\n",
    "for json_table in raw_jsons[1:]:\n",
    "    json_all = pd.concat([json_all,json_table], axis=0, ignore_index=True)\n",
    "json_all = json_all.drop_duplicates(subset=\"html_str\")\n",
    "del json_all[\"html_str\"]\n",
    "json_all.reset_index(drop=True, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output raw json without duplicate as dataframe in pickle\n",
    "with open('raw_json_no_dups','wb') as fout:\n",
    "    json_all.to_pickle(fout,compression=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting only user messages\n",
    "Also parsing links to message and authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to add empty string in case of empty search\n",
    "link_search = lambda s: '' if s is None else s['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap user messages \n",
    "def scrapFromHTML(soup, row):\n",
    "    row['author'] = \"https://opendatascience.slack.com\" + link_search(soup.find(\"a\", class_=\"c-link c-message__sender_link\"))\n",
    "    row['link'] = link_search(soup.find(\"a\", class_=\"c-link c-timestamp c-timestamp--static\"))\n",
    "    tag = soup.find(\"span\", class_=\"c-message__body\")\n",
    "    if not tag is None:\n",
    "        row['message'] = tag.get_text(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load raw table\n",
    "with open('raw_json_no_dups','rb') as fin:\n",
    "    html_table = pd.read_pickle(fin,compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add tables for future scrapped data\n",
    "html_table['author'] = \"\"\n",
    "html_table['link'] = \"\"\n",
    "html_table['message'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping of new information\n",
    "for i in html_table.index:\n",
    "    html_doc = html_table['html'][i]['name']\n",
    "    soup = BeautifulSoup(html_doc,'html5lib')\n",
    "    scrapFromHTML(soup, html_table.loc[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy table without non-user messages\n",
    "html_table_usr_msg = html_table[html_table.author!=\"https://opendatascience.slack.com\"]\n",
    "html_table_usr_msg.reset_index(drop=True, inplace=True)\n",
    "del html_table_usr_msg['html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output user messages table\n",
    "with open(\"user_messages\",'wb') as fout:\n",
    "    html_table_usr_msg.to_pickle(fout, compression=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting numbers out of text\n",
    "And removing messages without numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input user messages table\n",
    "with open(\"user_messages\",'rb') as fin:\n",
    "    usr_msg = pd.read_pickle(fin, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_msg['numbers'] = usr_msg.apply(lambda x: [], axis=1)\n",
    "usr_msg['hasNumbers'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#scrapping numbers in messages\n",
    "for i in usr_msg.index:\n",
    "    msg = usr_msg['message'][i]\n",
    "    usr_msg['numbers'][i] = re.findall(r'[0-9]+', msg)\n",
    "    if usr_msg['numbers'][i]:\n",
    "        usr_msg['hasNumbers'][i] = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy table without only-word messages (other are candidates for having vacancy information)\n",
    "usr_msg_clear = usr_msg[usr_msg.hasNumbers]\n",
    "usr_msg_clear.reset_index(drop=True, inplace=True)\n",
    "del usr_msg_clear['hasNumbers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output clear user messages table\n",
    "with open(\"user_messages_clear\",'wb') as fout:\n",
    "    usr_msg_clear.to_pickle(fout, compression=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing data and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input user messages table\n",
    "with open(\"user_messages_clear\",'rb') as fin:\n",
    "    usr_msg_clear = pd.read_pickle(fin, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SlackUser:\n",
    "    lmsu_url = 'https://lomonosov-msu.ru'\n",
    "\n",
    "    def __init__(self, filename='slack_dump.json'):\n",
    "        self.data = {}\n",
    "        self.filename = filename\n",
    "\n",
    "        self._session: Optional[aiohttp.ClientSession] = None\n",
    "\n",
    "    @property\n",
    "def session(self) -> aiohttp.ClientSession:\n",
    "if self._session is None:\n",
    "self._session = aiohttp.ClientSession()\n",
    "return self._session\n",
    "\n",
    "async def close(self):\n",
    "if self._session is not None:\n",
    "await self.session.close()\n",
    "\n",
    "def dump_exist(self, filename: str = None):\n",
    "return is_non_zero_file(filename or self.filename)\n",
    "\n",
    "def load(self, filename: str = None):\n",
    "with open(filename or self.filename, 'r', encoding='utf-8') as file:\n",
    "self.data = json.load(file)\n",
    "\n",
    "def dump(self, filename: str = None):\n",
    "with open(filename or self.filename, 'w', encoding='utf-8') as file:\n",
    "json.dump(self.data, file, indent=True, ensure_ascii=False)\n",
    "\n",
    "async def request(self, url, method='GET', data=None, headers=None):\n",
    "async with self.session.request(method, url, data=data, headers=headers) as response:\n",
    "return await response.read()\n",
    "\n",
    "async def authorization_on_msu(self, username, password):\n",
    "login_url = f'{self.lmsu_url}/rus/login'\n",
    "login_page = await self.request(login_url)\n",
    "csrf_token = BeautifulSoup(login_page, features='lxml').find('input', {'name': '_csrf_token'})['value']\n",
    "\n",
    "data = {'_username': username, '_password': password, '_remember_me': 'on', '_csrf_token': csrf_token}\n",
    "headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "await self.request(f'{self.lmsu_url}/login_check', method='POST', data=data, headers=headers)\n",
    "\n",
    "async def scrap_user(self, user_id):\n",
    "response = await self.request(f'{self.lmsu_url}/rus/user/achievement/user/{user_id}/list')\n",
    "soup = BeautifulSoup(response, features=\"lxml\")\n",
    "name_field = soup.find('h3', {'class': 'achievements-user__name'}) or \\\n",
    "soup.find('a', {'class': 'user-block__link user-block__link--logged'})\n",
    "user_data = {'name': name_field.text.strip(), 'achievements': []}\n",
    "subsection(f'Processing user \\\"{user_data[\"name\"]}\\\"')\n",
    "for achievement in soup.find_all(\"article\", {\"class\": \"achievement\"}):\n",
    "curr_data = {\n",
    "'title': achievement.find(\"a\", {\"class\": \"achievement__link\"}).text.strip(),\n",
    "'category': achievement.find(\"p\", {\"class\": \"achievement__more\"}).text.strip(),\n",
    "'score': int(achievement.find(\"span\", {\"class\": \"ach-pill\"}).text),\n",
    "'checked': bool(achievement.find(\"input\", {\"checked\": \"checked\"})),\n",
    "'url': self.lmsu_url + achievement.find(\"a\", {\"class\": \"achievement__link\"})['href'],\n",
    "'date': '',\n",
    "'file': '',\n",
    "'comment': '',\n",
    "'comment_our': '',\n",
    "}\n",
    "user_data['achievements'].append(curr_data)\n",
    "return user_id, user_data\n",
    "\n",
    "async def scrap_users(self, users_id):\n",
    "coros = [self.scrap_user(user_id) for user_id in users_id]\n",
    "for coro in asyncio.as_completed(coros):\n",
    "user_id, self.data[user_id] = await coro\n",
    "\n",
    "async def scrap_achievement(self, achievement):\n",
    "response = await self.request(achievement['url'])\n",
    "soup = BeautifulSoup(response, features=\"lxml\")\n",
    "for row in soup.find_all(\"div\", {\"class\": \"request__row\"}):\n",
    "if row.find(\"div\", {\"class\": \"request__row-title\"}).text.strip() == 'Дата получения':\n",
    "achievement['date'] = row.find(\"div\", {\"class\": \"request__row-info\"}).text.strip()\n",
    "if row.find(\"div\", {\"class\": \"request__row-title\"}).text.strip() == 'Дополнительно':\n",
    "achievement['comment'] = row.find(\"div\", {\"class\": \"request__row-info\"}).text.strip()\n",
    "file = soup.find(\"a\", {\"class\": \"file-list__file-name\"})\n",
    "achievement['file'] = self.lmsu_url + file.attrs['href'] if file else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
